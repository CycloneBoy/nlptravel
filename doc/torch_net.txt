BertPlome(
  (model): BertMeModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (py_ebd): Embedding(30, 32)
  (sk_ebd): Embedding(7, 32)
  (gru_py): GRU(
    (GRU_layer): GRU(32, 768, batch_first=True)
  )
  (gru_sk): GRU(
    (GRU_layer): GRU(32, 768, batch_first=True)
  )
  (hanzi_linear): Linear(in_features=768, out_features=21128, bias=True)
  (pinyin_linear): Linear(in_features=768, out_features=430, bias=True)
  (log_softmax): LogSoftmax(dim=-1)
  (softmax): Softmax(dim=-1)
)
torch:
model.embeddings.word_embeddings.weight torch.Size([21128, 768])
model.embeddings.position_embeddings.weight torch.Size([512, 768])
model.embeddings.LayerNorm.weight torch.Size([768])
model.embeddings.LayerNorm.bias torch.Size([768])
model.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.0.attention.self.query.bias torch.Size([768])
model.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.0.attention.self.key.bias torch.Size([768])
model.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.0.attention.self.value.bias torch.Size([768])
model.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.0.attention.output.dense.bias torch.Size([768])
model.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.0.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.0.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.0.output.dense.bias torch.Size([768])
model.encoder.layer.0.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.0.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.1.attention.self.query.bias torch.Size([768])
model.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.1.attention.self.key.bias torch.Size([768])
model.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.1.attention.self.value.bias torch.Size([768])
model.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.1.attention.output.dense.bias torch.Size([768])
model.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.1.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.1.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.1.output.dense.bias torch.Size([768])
model.encoder.layer.1.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.1.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.2.attention.self.query.bias torch.Size([768])
model.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.2.attention.self.key.bias torch.Size([768])
model.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.2.attention.self.value.bias torch.Size([768])
model.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.2.attention.output.dense.bias torch.Size([768])
model.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.2.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.2.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.2.output.dense.bias torch.Size([768])
model.encoder.layer.2.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.2.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.3.attention.self.query.bias torch.Size([768])
model.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.3.attention.self.key.bias torch.Size([768])
model.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.3.attention.self.value.bias torch.Size([768])
model.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.3.attention.output.dense.bias torch.Size([768])
model.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.3.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.3.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.3.output.dense.bias torch.Size([768])
model.encoder.layer.3.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.3.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.4.attention.self.query.bias torch.Size([768])
model.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.4.attention.self.key.bias torch.Size([768])
model.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.4.attention.self.value.bias torch.Size([768])
model.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.4.attention.output.dense.bias torch.Size([768])
model.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.4.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.4.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.4.output.dense.bias torch.Size([768])
model.encoder.layer.4.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.4.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.5.attention.self.query.bias torch.Size([768])
model.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.5.attention.self.key.bias torch.Size([768])
model.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.5.attention.self.value.bias torch.Size([768])
model.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.5.attention.output.dense.bias torch.Size([768])
model.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.5.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.5.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.5.output.dense.bias torch.Size([768])
model.encoder.layer.5.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.5.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.6.attention.self.query.bias torch.Size([768])
model.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.6.attention.self.key.bias torch.Size([768])
model.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.6.attention.self.value.bias torch.Size([768])
model.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.6.attention.output.dense.bias torch.Size([768])
model.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.6.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.6.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.6.output.dense.bias torch.Size([768])
model.encoder.layer.6.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.6.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.7.attention.self.query.bias torch.Size([768])
model.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.7.attention.self.key.bias torch.Size([768])
model.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.7.attention.self.value.bias torch.Size([768])
model.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.7.attention.output.dense.bias torch.Size([768])
model.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.7.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.7.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.7.output.dense.bias torch.Size([768])
model.encoder.layer.7.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.7.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.8.attention.self.query.bias torch.Size([768])
model.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.8.attention.self.key.bias torch.Size([768])
model.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.8.attention.self.value.bias torch.Size([768])
model.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.8.attention.output.dense.bias torch.Size([768])
model.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.8.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.8.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.8.output.dense.bias torch.Size([768])
model.encoder.layer.8.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.8.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.9.attention.self.query.bias torch.Size([768])
model.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.9.attention.self.key.bias torch.Size([768])
model.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.9.attention.self.value.bias torch.Size([768])
model.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.9.attention.output.dense.bias torch.Size([768])
model.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.9.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.9.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.9.output.dense.bias torch.Size([768])
model.encoder.layer.9.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.9.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.10.attention.self.query.bias torch.Size([768])
model.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.10.attention.self.key.bias torch.Size([768])
model.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.10.attention.self.value.bias torch.Size([768])
model.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.10.attention.output.dense.bias torch.Size([768])
model.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.10.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.10.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.10.output.dense.bias torch.Size([768])
model.encoder.layer.10.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.10.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])
model.encoder.layer.11.attention.self.query.bias torch.Size([768])
model.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])
model.encoder.layer.11.attention.self.key.bias torch.Size([768])
model.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])
model.encoder.layer.11.attention.self.value.bias torch.Size([768])
model.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])
model.encoder.layer.11.attention.output.dense.bias torch.Size([768])
model.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])
model.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])
model.encoder.layer.11.intermediate.dense.bias torch.Size([3072])
model.encoder.layer.11.output.dense.weight torch.Size([768, 3072])
model.encoder.layer.11.output.dense.bias torch.Size([768])
model.encoder.layer.11.output.LayerNorm.weight torch.Size([768])
model.encoder.layer.11.output.LayerNorm.bias torch.Size([768])
model.pooler.dense.weight torch.Size([768, 768])
model.pooler.dense.bias torch.Size([768])
py_ebd.weight torch.Size([30, 32])
sk_ebd.weight torch.Size([7, 32])
gru_py.GRU_layer.weight_ih_l0 torch.Size([2304, 32])
gru_py.GRU_layer.weight_hh_l0 torch.Size([2304, 768])
gru_py.GRU_layer.bias_ih_l0 torch.Size([2304])
gru_py.GRU_layer.bias_hh_l0 torch.Size([2304])
gru_sk.GRU_layer.weight_ih_l0 torch.Size([2304, 32])
gru_sk.GRU_layer.weight_hh_l0 torch.Size([2304, 768])
gru_sk.GRU_layer.bias_ih_l0 torch.Size([2304])
gru_sk.GRU_layer.bias_hh_l0 torch.Size([2304])
hanzi_linear.weight torch.Size([21128, 768])
hanzi_linear.bias torch.Size([21128])
pinyin_linear.weight torch.Size([430, 768])
pinyin_linear.bias torch.Size([430])